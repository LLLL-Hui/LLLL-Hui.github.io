
<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">


<meta name="author" content="Han Bao">



<meta name="description" content="Han Bao&#39;s Website">



<link rel="icon" href="/favicon.ico">



<meta name="keywords" content=" computer science  machine learning  statistics ">




<script>
  
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
      processEscapes: true,
      processEnvironments: true,
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    }
  };
</script>

<script async defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>




<link rel="canonical" href="https://hermite.jp/">


<link rel="alternate" type="application/rss+xml" href="https://hermite.jp/index.xml" title="Han Bao" />


<title>Hui Li</title>



<link media="screen" rel="stylesheet" href='https://hermite.jp/css/common.css'>
<link media="screen" rel="stylesheet" href='https://hermite.jp/css/content.css'>
<link media="screen" rel="stylesheet" href='https://hermite.jp/css/highlight.css'>

  <link rel="stylesheet" href='https://hermite.jp/css/index.css'>
</head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1>
    <a href="./index.html">Hui Li</a>
  </h1>

  <nav>
    
    <span class="nav-bar-item">
      <a class="link" href="./publication/index.html">Publication</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="./talk/index.html">Talk</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="./misc/index.html">Misc</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="./post/index.html">Post</a>
    </span>
    
  </nav>
</header>

    <main id="main" class="index">
        <div class="content">
<h3 id="about-me">About me</h3>
<img id="profile-image" src="./file/profile.jpg" width=150px />
<div id="profile">
    <li class="nostyle icon-misc">Assistant Professor in <a href="https://www.hakubi.kyoto-u.ac.jp/eng">the Hakubi Project</a> and <a href="https://www.i.kyoto-u.ac.jp/en/">Graduate School of Informatics</a>, <a href="https://www.kyoto-u.ac.jp/en">Kyoto University</a></li>
    <li class="nostyle icon-people">Member of <a href="http://www.ml.ist.i.kyoto-u.ac.jp/en/">Kashima-Yamada Lab</a></li>
    <li class="nostyle icon-mail">bao * i.kyoto-u.ac.jp</li>
    <li class="nostyle icon-file"><a href="/file/cv.pdf">CV</a>, <a href="https://scholar.google.com/citations?user=MqMzjeMAAAAJ">Google Scholar</a>, <a href="https://dblp.uni-trier.de/pid/120/1444-2.html">DBLP</a>, <a href="https://orcid.org/0000-0002-4473-2604">ORCID</a>, <a href="https://researchmap.jp/hanbao">researchmap</a>, <a href="https://github.com/levelfour">GitHub</a></li>
</div>
<hr>
<h3 id="research-interests">Research interests</h3>
<p>My central focus of research is in theoretical understanding of statistical machine learning, particularly from the following perspectives.</p>
<ol>
<li>Learning theory of <em>loss functions</em>, through which I like to study robustness to adversarial attacks (<a href="http://proceedings.mlr.press/v125/bao20a.html">COLT2020</a>) and class imbalance (<a href="http://proceedings.mlr.press/v108/bao20a.html">AISTATS2020</a>, <a href="http://proceedings.mlr.press/v130/bao21b.html">AISTATS2021</a>).</li>
<li><em>Evaluation metrics</em> of predictions and representations. Recently, I am interested in how it is possible to learn good representations via similarity in light of a downstream task (<a href="http://proceedings.mlr.press/v80/bao18a.html">ICML2018</a>, <a href="https://proceedings.mlr.press/v151/bao22a.html">AISTATS2022</a>, <a href="https://proceedings.mlr.press/v162/bao22e.html">ICML2022</a>).</li>
</ol>
<p>I am glad to have discussions with those who have common interests!
You may have a look at the slides of my past talks such as <a href="/slides/202007_KyotoU.pdf">this</a> to see my tastes.</p>
<hr>
<h3 id="news">News</h3>
<ul>
<li>Aug 23, 2022: Our monograph <a href="https://mitpress.mit.edu/books/machine-learning-weak-supervision">&ldquo;Machine Learning from Weak Supervision: An Empirical Risk Minimization Approach&rdquo;</a> has been published from MIT press.</li>
<li>May 16, 2022: Our paper &ldquo;On the Surrogate Gap between Contrastive and Supervised Losses&rdquo; has been accepted by ICML2022. We improve upper and lower bounds for the gap between contrastive and supervised losses and claim that larger negative samples are good for downstream classification. The earlier version is available <a href="https://arxiv.org/abs/2110.02501">here</a>.</li>
<li>May 9, 2022: Our AISTATS2022 paper “Pairwise Supervision Can Provably Elicit a Decision Boundary” has appeared in the proceedings (<a href="https://proceedings.mlr.press/v151/bao22a.html">link</a>).</li>
<li>Apr 1, 2022: I have joined Kyoto University as an assistant professor. Feel free to visit Kyoto and contact me.</li>
<li>Mar 24, 2022: I finished my three year PhD in computer science and nine years life in the University of Tokyo. Also, I was fortunate to have an opportunity to be a representative graduate at <a href="https://www.u-tokyo.ac.jp/focus/ja/articles/z1301_00056.html">the diploma presentation ceremony</a> to make an address in the ceremony. My student life has been supported by so many great friends not only in Tokyo but also in other cities in Japan and even in overseas. I would like to appreciate everyone who has been with me!</li>
</ul>
<details>
<summary>Show more</summary>
<ul>
<li>Jan 19, 2022: Our paper &ldquo;Pairwise Supervision Can Provably Elicit a Decision Boundary&rdquo; has been accepted by AISTATS2022. We elucidated that pairwise supervision (i.e., information indicating whether two input vectors belong to the same underlying class) is sufficient to recover a binary decision boundary. The latest version is available <a href="https://arxiv.org/abs/2006.06207">here</a> (updated on Mar 3).</li>
<li>Jun 21, 2021: Our paper <a href="https://arxiv.org/abs/2002.00995">&ldquo;Learning from Noisy Similar and Dissimilar Data&rdquo;</a> has been accepted by ECMLPKDD2021.</li>
<li>May 17, 2021: We have publicized a <a href="https://arxiv.org/abs/2005.13748">corrigendum</a> to our COLT2020 paper. The definition of calibrated losses is corrected and the proofs of our main results are modified.</li>
<li>Jan 23, 2021: Our paper <a href="http://proceedings.mlr.press/v130/bao21b.html">&ldquo;Fenchel-Young Losses with Skewed Entropies for Class-posterior Probability Estimation&rdquo;</a> has been accepted by AISTATS2021!</li>
<li>Jan 8, 2021: Our presentation at IBIS2020 got the best presentation award (1st place out of 116 presentations)!</li>
</ul>
</details>
</div>

    </main>
    <footer id="footer">
  <div>
    <span>© Han Bao / Last updated: 2022-08-29</span>
  </div>

  <div class="footnote">
    <span></span>
  </div>
</footer>

  </div>
  

<link media="screen" rel="stylesheet" href="https://hermite.jp/css/main.css" />





</body>

</html>
